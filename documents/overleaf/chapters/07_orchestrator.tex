\chapter{QoS-Monitor \& -Orchestrator\label{cha:orchestrator}}

The algorithm described in section \ref{cha:algorithm} works so far in a static context. However, the task is to apply the optimal deployment to a Node-RED cluster, where the network conditions can change at any time. While the application characteristics remain unchanged, the infrastructure can change mainly in two ways. First, fog nodes may join or leave the network, and second, network connection quality between nodes can get better or worse. These changes must be detected by the Orchestrator, and if the current deployment strategy no longer meets the application requirements, a new optimal deployment must be found and applied.

The class \texttt{NodeRedOrchestrator} takes care of this job. It monitors the infrastructure and deploys the optimal deployment to it by using the \textit{QoS-Scheduler}. To make use of the \textit{QoS-Scheduler}, an \texttt{Infrastructure} and \texttt{Application} instance must be available. The former one is actively maintained by the Orchestrator (see section \ref{sec:orchestrator-monitoring}), while the latter one is statically created upon start-up of the \texttt{NodeRedOrchestrator}. The class \texttt{FogNode} of the algorithm in section \ref{cha:algorithm} is extended by the class \texttt{NodeRedFogNode} in order to add additional functionality for measuring the nodes hardware and network capabilities before it is added to the infrastructure.

The corresponding class diagram is shown in figure \ref{fig:orchestrator-classdisgram}.

\section{Monitoring the infrastructure\label{sec:orchestrator-monitoring}}

Directly after starting the orchestrator, it is only aware of the applications that should be deployed to the infrastructure. However, the infrastructure is empty at the beginning. Fog nodes send their availability status via a \texttt{heartbeat} message to a \texttt{MQTT} broker at a given interval. The orchestrator listens for those messages and has different handlers for different possibilities:
\begin{enumerate}
    \item The node sending the heartbeat is not registered in the current infrastructure\\
    → Handle new fog node (see section \ref{sec:handling-new-fog-nodes})
    \item The node sending the heartbeat is registered\\
    → Update timestamp of last received heartbeat
    \item Orchestrator is expecting but missing a heartbeat from a node\\
    → Check if node is still up and if not, remove it from infrastructure
\end{enumerate}

\section{Handling new fog nodes\label{sec:handling-new-fog-nodes}}

In the case of receiving a heartbeat from a node that the orchestrator does not know yet, the hardware capabilities as well as the network connection of the node must be checked before it can be added to the infrastructure. To achieve this, the node is able to receive several commands via \texttt{MQTT}.

\begin{itemize}
    \item \texttt{sysinfo}: Sends basic information about the nodes hardware (\textit{RAM}, \textit{storage}, \textit{cpu cores}, \textit{connected hardware}).
    \item \texttt{ping}: Measures the \textit{RTT} from this node to another node. Returns the latency in milliseconds. The command line tool \texttt{ping} is used for this purpose.
    \item \texttt{bandwidth}: Measures the network connection from this node to another node. Returns the bandwidth in Mbit/s.
    \item \texttt{benchmark\_cpu}: Runs the tool \texttt{sysbench} which is a lightweight benchmark tool made for linux servers. The result is the \textit{execution time} for a given (but always the same) task. A CPU score is calculated based on the execution time (see section \ref{sec:benchmark-cpu}). This score is then returned by the \texttt{benchmark\_cpu} command.
\end{itemize}

Figure \ref{fig:orchestrator-initial-heartbeat} shows how the Orchestrator handles a \texttt{heartbeat} from an unknown fog node. Note that \texttt{ping} and \texttt{bandwidth} are measured from this node to \textit{all other existing nodes} in the infrastructure.

\begin{figure}[htb]
    \centering
    \includegraphics[width=0.5\textwidth]{orchestrator-initial-heartbeat}
    \caption{Sequence diagram of the orchestrator handling a new heartbeat}
    \label{fig:orchestrator-initial-heartbeat}
\end{figure}

After the Orchestrator has collected all required information about the node, it is added to the infrastructure and the \textit{Qos Scheduler} is run to (eventually) find a new optimal deployment.

\subsection{Measuring bandwidth\label{sec:measuring-bandwidth}}
To measure the bandwidth a tool called \texttt{iperf3} was used first, which measures the transfer rate via \texttt{TCP}. However, these measured values could not be achieved in practice, since \texttt{HTTP} is used for message transmission between the modules. Because \texttt{HTTP} is build on top of \texttt{TCP}, it introduces additional protocol overhead and thus has a lower transfer rate than \texttt{TCP}. For this reason, \texttt{iperf3} was replaced by a measurement method using \texttt{HTTP}:

To measure the uplink from \textit{node A} to \textit{node B}, \textit{node A} sends a \texttt{HTTP POST} request to a predefined endpoint on \textit{node B}. The \texttt{HTTP body} initially contains \texttt{1MB} of data. After \textit{node B} has received the request, it responds with an empty body. \textit{Node A} can then calculate the \textit{transfer rate} by using the \textit{data size} and \textit{transfer time}.
If the response from node B is received fairly rapidly (less than 500ms), node A repeats the measurement with a larger data size to get a more accurate result.

\subsection{Benchmarking CPU\label{sec:benchmark-cpu}}
The algorithm uses the unit \textit{MIPS} to specify the speed of a CPU, respectively to specify the required CPU instructions of a module in order to calculate the execution time of a module on a node. However, \textit{MIPS} can't be read from the system like total RAM or amount of CPU cores. For this reason a CPU score is used instead. The Orchestrator calculates this score according to the following formula:
\[\textrm{CPU score} = \frac{10000}{\textrm{sysbench result [time in ms]}}\]

Thus, the lower the execution time of \texttt{sysbench}, the higher the CPU score. This score is used for the field \texttt{cpuMips} of a \texttt{FogNode} instance.

However, the field \texttt{requiredMi} of an \texttt{AppSoftwareModule} instance must also be set accordingly. To determine this value, the corresponding software module is deployed on a node where we know the CPU score. Since we can measure the execution time of processing one message on that node, the required value can be calculated as follows:
\[\textrm{\texttt{requiredMi}} = \textrm{CPU score} \boldsymbol{\cdot} \frac{\textrm{execution time [ms]}}{1000 \textrm{ [ms]}}\]


\section{Using the QoS-Scheduler in the Orchestrator}

The output of the algorithm tells which module should be deployed on which node. In the example shown in table \ref{tab:deployment-strategy-example}, the algorithm has decided to distribute the three modules of the object detection application (see figure \ref{fig:object-detection-appmodules}) over two nodes (\textit{node A} and \textit{node B}).

\begin{table}[htb]
    \centering
    \begin{tabular}{|m{1.5cm}|m{4cm}|m{3.5cm}|}
        \hline
        \textbf{Order} & \textbf{Module} & \textbf{Executing node}\\
        \hline
        1 & \texttt{camera-controller} & node A\\
        \hline
        2 & \texttt{object-detector} & node B\\
        \hline
        3 & \texttt{image-viewer} & node A\\
        \hline
    \end{tabular}
    \caption{Sample deployment strategy for object detection application}
    \label{tab:deployment-strategy-example}
\end{table}

\subsection*{Node-RED Controller}

Every running Node-RED instance can be controlled via the \textit{Node-RED Admin API}\footnote{https://nodered.org/docs/api/admin/methods/}. In order to make use of it, the \texttt{NodeRedController} was implemented (see figure \ref{fig:orchestrator-classdisgram}). It is able to call all necessary API functions, so that the Orchestrator can control the Node-RED instances (create, update and delete flows in particular).

\subsection*{Flow database}
In a Node-RED context, each module is a Node-RED flow and is stored in a database. A node-RED flow in turn can be exported, saved and imported as JSON. However, in this architecture, nothing is manually exported and stored in a database. Instead, there is a dedicated Node-RED instance which serves exclusively as flow database and is not used for task execution. The Orchestrator can then query a specific flow from this instance via the \texttt{NodeRedController} by calling the method \texttt{getFlowByName(flowName)}.

The flow database is implemented in the class \texttt{NodeRedFlowDatabase} (see figure \ref{fig:orchestrator-classdisgram}).

\subsection*{Deploying flows}
The orchestrator fetches each flow from the database and deploys it to the node-RED instance running on the scheduled fog node. The orchestrator manages the flows on the nodes by using the \texttt{NodeRedController}. The process of deploying a deployment strategy (instance of \texttt{AppDeployment}) to the infrastructure is shown in figure \ref{fig:orchestrator-activitydiagram-deploy-flows}.

\begin{figure}[htb]
    \centering
    \includegraphics[width=0.7\textwidth]{orchestrator-activitydiagram-deploy-flows}
    \caption{Activity diagram for deploying an \texttt{AppDeployment} to a Node-RED infrastructure}
    \label{fig:orchestrator-activitydiagram-deploy-flows}
\end{figure}

\subsection*{Communication}
The communication between two application modules (Node-RED flows) is done by using the \textit{HTTP nodes} available in Node-RED. For instance, the module \texttt{object-detector} is accepting HTTP requests on the endpoint \texttt{/object-detection/object-detector} (module \textit{input}), so that the module \texttt{camera-controller} (the previous module in the loop) can send its \textit{output} via an \texttt{HTTP POST} request to that endpoint. However, \textit{node A} must know that the output of \texttt{camera-controller} must be send to \textit{node B}. To achieve this, the Orchestrator places the address of \textit{node B} in the \texttt{camera-controller} flow it got from the database before it deploys this flow to \textit{node A} (see figure \ref{fig:orchestrator-activitydiagram-deploy-flows}).

\section{Monitoring the current deployment strategy}

The Orchestrator must verify that the selected deployment strategy really meets the application requirements. Since the goal here is to get the result within a certain time, the last module in the loop must tell the Orchestrator how long it took to process the application loop. If the latency requirements could not be fulfilled, the Orchestrator must be able to identify the problem and react to the environmental changes.

To realize this, every application module attaches the current time \textit{before} and \textit{after} processing to the message object (or in other words: \textit{after} receiving the message, and \textit{before} sending a new message to the next module / node). From this values, the actual processing time on a node, as well as the transfer time for every message between two nodes can be calculated. Where the timestamps are taken and how these are used to calculate the individual processing and transfer times is illustrated in a timeline in figure \ref{fig:orchestrator-statistics-timeline}.

\begin{figure}[h]
    \centering
    \includegraphics[width=1.0\textwidth]{orchestrator-statistics-timeline}
    \caption{Timeline for executing the loop of the object detection application}
    \label{fig:orchestrator-statistics-timeline}
\end{figure}

The calculates values are stored in a \texttt{statistics} object in the message and passed through to the next module. The last module in the loop sends this object to the Orchestrator via \texttt{MQTT}, which can then further analyze this object.

The \texttt{statistics} object contains two arrays:
\begin{enumerate}
    \item \texttt{transfers} for all message transfers between nodes
    \item \texttt{processes} for all task executions on the nodes
\end{enumerate}

An example is shown in listing \ref{lst:statistics}.

\lstinputlisting[language=json, caption=Sample statistics JSON object,label={lst:statistics}]{listings/statistics.json}

Not only the entire execution time of a loop can be extracted from that object, but also which part took how long. In the case of a downgrade of one network connection, it is possible to determine between which nodes the connection has downgraded. Since we know the \textit{size}, \textit{transfer time}, \textit{source node} and \textit{destination node}, the current bandwidth between those nodes can be calculated and the infrastructure can be updated accordingly. The Orchestrator can then run the algorithm with the updated values to see if there is a new optimal deployment.